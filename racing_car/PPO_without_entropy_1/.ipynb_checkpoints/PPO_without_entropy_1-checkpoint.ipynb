{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e2cbf3-664a-4c74-9f2b-d958833f0d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO on CarRacing: 100%|██████████████████████████████████████████████████| 100/100 [4:13:43<00:00, 152.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -83.85 ± 1.15\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "log_dir = \"./ppo_carracing_tensorboard/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "    env = Monitor(env)  \n",
    "    env.reset(seed=SEED)\n",
    "    env.action_space.seed(SEED)\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecTransposeImage(env)  \n",
    "\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    seed=SEED,\n",
    "    verbose=0, \n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0,\n",
    "    tensorboard_log=log_dir,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "total_timesteps = 1_000_000\n",
    "chunk_size = 10_000\n",
    "\n",
    "with trange(0, total_timesteps, chunk_size, desc=\"Training PPO on CarRacing\") as pbar:\n",
    "    for _ in pbar:\n",
    "        model.learn(\n",
    "            total_timesteps=chunk_size,\n",
    "            reset_num_timesteps=False,\n",
    "            tb_log_name=\"PPO_CarRacing_Entropy\",\n",
    "            log_interval=1 \n",
    "        )\n",
    "\n",
    "model.save(\"ppo_carracing_sb3\")\n",
    "model = PPO.load(\"ppo_carracing_sb3\", env=env)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, render=True, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0988bb-7161-44e6-b63b-2e8129fc2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall average across seeds: Mean reward = -83.85 ± 1.15\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "import numpy as np\n",
    "\n",
    "seeds = [0, 1, 10, 42, 100, 123, 999]\n",
    "n_eval_episodes = 5\n",
    "\n",
    "def make_env(seed):\n",
    "    def _init():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "mean_rewards = []\n",
    "std_rewards = []\n",
    "\n",
    "for seed in seeds:\n",
    "    vec_env = DummyVecEnv([make_env(seed)])\n",
    "    vec_env = VecTransposeImage(vec_env)\n",
    "    model = PPO.load(\"ppo_carracing_sb3\", env=vec_env)\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, vec_env, n_eval_episodes=n_eval_episodes, render=False, deterministic=False\n",
    "    )\n",
    "    mean_rewards.append(mean_reward)\n",
    "    std_rewards.append(std_reward)\n",
    "\n",
    "overall_mean = np.mean(mean_rewards)\n",
    "overall_std = np.mean(std_rewards)\n",
    "\n",
    "print(f\"\\nOverall average across seeds: Mean reward = {overall_mean:.2f} ± {overall_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22cf10ef-41ff-4b8c-8b7a-40821a3fa8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: -85.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     27\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.03\u001b[39m)  \u001b[38;5;66;03m# slow down to make it watchable\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "import time\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"human\", continuous=True)  # human mode = display in real time\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "model = PPO.load(\"ppo_carracing_sb3\", env=env)\n",
    "\n",
    "n_episodes = 5\n",
    "for ep in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward[0]\n",
    "        time.sleep(0.03)  \n",
    "\n",
    "    print(f\"Episode {ep + 1} reward: {total_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f19c4-a0f0-4b4d-a6e1-04da42093d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
